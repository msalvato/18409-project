%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Short Sectioned Assignment
% LaTeX Template
% Version 1.0 (5/5/12)
%
% This template has been downloaded from:
% http://www.LaTeXTemplates.com
%
% Original author:
% Frits Wenneker (http://www.howtotex.com)
%
% License:
% CC BY-NC-SA 3.0 (http://creativecommons.org/licenses/by-nc-sa/3.0/)
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%----------------------------------------------------------------------------------------
%	PACKAGES AND OTHER DOCUMENT CONFIGURATIONS
%----------------------------------------------------------------------------------------

\documentclass[paper=a4, fontsize=10.5pt]{scrartcl} % A4 paper and 11pt font size

\usepackage[T1]{fontenc} % Use 8-bit encoding that has 256 glyphs


%\usepackage{fourier} % Use the Adobe Utopia font for the document - comment this line to return to the LaTeX default
\usepackage[english]{babel} % English language/hyphenation
\usepackage{amsmath,amsfonts,amsthm} % Math packages
\usepackage{mathtools}
\DeclarePairedDelimiter\ceil{\lceil}{\rceil}
\DeclarePairedDelimiter\floor{\lfloor}{\rfloor}
\usepackage{lipsum} % Used for inserting dummy 'Lorem ipsum' text into the template
\usepackage[margin=1.25in]{geometry}


\usepackage{sectsty} % Allows customizing section commands
\allsectionsfont{\centering \normalfont\scshape} % Make all sections centered, the default font and small caps

\usepackage{fancyhdr} % Custom headers and footers
\pagestyle{fancyplain} % Makes all pages in the document conform to the custom headers and footers
\fancyhead{} % No page header - if you want one, create it in the same way as the footers below
\fancyfoot[L]{} % Empty left footer
\fancyfoot[C]{} % Empty center footer
\fancyfoot[R]{\thepage} % Page numbering for right footer
\renewcommand{\headrulewidth}{0pt} % Remove header underlines
\renewcommand{\footrulewidth}{0pt} % Remove footer underlines
\setlength{\headheight}{13.6pt} % Customize the height of the header

\numberwithin{equation}{section} % Number equations within sections (i.e. 1.1, 1.2, 2.1, 2.2 instead of 1, 2, 3, 4)
\numberwithin{figure}{section} % Number figures within sections (i.e. 1.1, 1.2, 2.1, 2.2 instead of 1, 2, 3, 4)
\numberwithin{table}{section} % Number tables within sections (i.e. 1.1, 1.2, 2.1, 2.2 instead of 1, 2, 3, 4)

\setlength\parindent{0pt} % Removes all indentation from paragraphs - comment this line for an assignment with lots of text

\usepackage{mathrsfs}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{bbm}
\usepackage{amssymb} 
\usepackage[mathscr]{euscript}
\usepackage{parskip}


\renewcommand{\vec}[1]{\mathbf{#1}}
\newcommand{\vecsf}[1]{\textsf{\textbf{#1}}}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}

%----------------------------------------------------------------------------------------
%	TITLE SECTION
%----------------------------------------------------------------------------------------

\newcommand{\horrule}[1]{\rule{\linewidth}{#1}} % Create horizontal rule command with 1 argument of height

\title{	
\normalfont \normalsize 
\textsc{18.409 - Algorithmic Aspects of Machine Learning} \\ [25pt] % Your university, school and/or department name(s)
\horrule{0.5pt} \\[0.4cm] % Thin top horizontal rule
\huge Final Project \\ % The assignment title
\horrule{2pt} \\[0.5cm] % Thick bottom horizontal rule
}

\author{Mike Salvato and Andrew Spielberg} % Your name

\date{\normalsize\today} % Today's date or a custom date

\begin{document}

\maketitle % Print the title

%----------------------------------------------------------------------------------------
%	PROBLEM 1
%----------------------------------------------------------------------------------------
\section{Introduction}

Here we review work done on approximations for clustering algorithms, focusing on the work of \cite{firstpaper} and \cite{secondpaper}. The essential goal is that for various data points about sets of interest, we wish to group those data points by which set they belong to. For example we may have a collection of face images, generate a relatively low dimensional datapoint (feature vector) for each image, and then cluster this data to try to determine whether images came from the same people (each cluster corresponding to a distinct person). However the problem of clustering points given a distance metric is in general NP-hard. Additionally, analysis of clustering alone does not take into account the relationship between the clusters and the underlying sets they belong to. For example, even if the images were clustered optimally, according to some objective function of distance between datapoints, that clustering may not correctly group the images by which person's face is in the image. The work here will focus on algorithms for approximating correct clustering based on the underlying sets.

\subsection{Background}

In order to begin our analysis of clustering, we first define what clustering means exactly, and the relationship it holds in solving an underlying data analysis problem. 

\subsection{k-medians}
The algorithm we will focus on is the k-medians algorithm. The k-medians algorithm assumes a space $(X, d)$ for which $X$ is the set of possible points and distance $d : ({ X \choose 2}) \rightarrow R_{\geq 0}$ which satisfies the triangle inequality. We are then given a set of points $S \subset X$. The goal is to label the points in $S$ each into one of k clusters $C_1, C_2, ..., C_k$ with corresponding representative points $c_1, c_2, ..., c_k \in X$. This defines a clustering $C$. The optimal objective clustering is that which minimizes $\Phi(C) = \sum\limits_{i = 1}^k \sum_{x \in C_i} d(x, c_i)$.

Let the distance between two clusterings $dist(C, C')$ be the 
fraction of points for which those two clusterings disagree on labels under the minimizing bijection between cluster labels, and let $\text{OPT}_\Phi = \Phi(C^*)$

\subsection{($c, \delta$)-property}
Thus far we have only addressed clustering based on the data points, but not the underlying sets of interest.

The \textbf{objective} function $\Phi(C)$ is the cost of a clustering based only on the distance between points. However the optimal \textbf{target} clustering is that which partitions the points best based on the underlying sets the data points are drawn from. In the ideal problem statement the optimal objective clustering $C^*$ and the optimal target clustering $C^T$ are the same. However in practice this is not usually the case. In order to be able to perform analysis on the quality of a clustering \cite{firstpaper} define the $(c, \delta)$-property.  If a clustering satisfies the $(c, \delta)$-property, then for all clusterings where $\Phi(C) \leq c*\text{OPT}_\Phi$, $C$ is also $\delta$-close to $C^T$. By analyzing problem instances where this holds true for some values of $c$ and $\delta$, we can generate approximate clustering schema to achieve approximation solutions to discovering the target clustering.

 A couple of useful properties:

\begin{itemize}

\item In the case where the target clustering $C^T$ isn't the optimal clustering $C^*$, these two clusterings are $\delta$-close.

\item Since $dist$ is a metric, it satisfies the triangle inequality, and therefore any clustering that that is within $(c, \delta)$ wrt $C_T$ also satisfies $(c, 2\delta)$ of the optimal clustering. %(TODO: explain out with math?) I don't think it's necessary.



\end{itemize}

% Do we need all these? They're basically the strawmen, so I'm not sure they're the best use of space.


%Following this section begins specific analyses of how the $(c, \delta)$ assumption leads to efficient algorithms for clustering for three commonly used objective functions ($k$-median, $k$-mean, $k$-min-sum).  However, before that, it's important to understand three very important limitations and implications of this setting.
%
%\begin{enumerate}
%\item $(c, \delta)$ properties do not imply $(kc, k\delta)$ properties in general for any of the three objective functions we'll analyze here.  To see this, let's look at a counterexample for the $k$-median example (TODO: should I generalize this?).  Let $1 \leq c_1 < c_2 $, and $\delta, \alpha > 0$.  Let the target clustering consist of a single cluster with $n(1 - 2\alpha$ points at a single location, and the remaining $k-1$ clusters each with $2\frac{\alpha n}{k-1}$ all at distance $1$ (TODO: draw a picture).  Each point in $C_1$ is greater than $c_2 n$ away from any other cluster.  Note that this clustering has objective $2\alpha n - (k - 1)$, in other words, all but one point in each cluster but $C_1$ contributes $1$ to the objective ($C_1$ contributes nothing since each point lies on the median).  Then, consider an alternate $c_2$ solution which splits $C_1$ into two clusters and merges clusters $C_2$ and $C_3$.  Since $n(\frac{1}{2} - \alpha)$ points from the first cluster are then misclassified (as it was split in two), clearly this has approximation error at least 
%$\frac{1}{n}$ of that, and in fact, strictly greater.  If, in our original setup, we set the distance between points \emph{between} any two clusters to be $D > 1$ (which is defined to give this latter clustering a $c_2 OPT$ objective value).  But then, consider that any $c_1$ approximation cannot merge clusters (TODO: note that c2 was defined as the minimum that has a merging of clusters, can be seen by symmetry)).  error on fewer than $\frac{2\alpha n}{k-1} < \delta n$ (TODO: why?). Therefore, this is less than $k$ when we set $\alpha$ (TODO: explain that this defines the family) such that $k > 1 + \frac{2\alpha}{\delta} $.
%
%TODO: draw this whole proof.
%
%
%
%\item $c$-approximations under the $(c, \delta)$ assumption for $c > 1$ are as hard as finding a $c$-approximation in general. TODO: proof
%
%\item Finally, finding a $c$-approximation under the $(1, \delta)$ assumption is NP-Hard. TODO: proof
%
%\end{enumerate}

\subsection{Goals}

The goal of this work is to develop a polynomial runtime clustering algorithm with minimal constraints on the values of $c$ and $\delta$ as described above. It is known that a better than $1 + \frac{2}{\delta}$ approximation for k-medians is NP-hard \cite{JMS02}. In this review we will show explore the following results:

\begin{itemize}

\item Assume we have a problem with a $(1+\alpha, \delta)$-property, and that all target clusters have size greater than $\delta n$. There exists a PTAS $\delta$-close to the target and an algorithm for finding this clustering.

\item Assume that the (k-1)-medians optimal cluster is more expensive than the k optimal clustering by a factor of $(1+\alpha)$ where $\alpha > 0$. Then it is possible to achieve a ($1+\epsilon$) approximation to the optimal k-medians in time polynomial in $n$ and $k$, and exponential in $1/\epsilon$ and $1/\alpha$, where $n = |S|$.

\item Assume we have a problem with a $(1+\alpha, \delta)$-property, with no restriction on cluster size. There exists n $O(\delta/\alpha)$-close to the target and an algorithm for finding this clustering.
\end{itemize}


\section{Stability yields a PTAS for k-Median and k-Means Clustering}

This paper looks at a few statistics about how points are distributed and how it relates to optimal clusterings.  In particular, it studies notions of stability, which describes bounds on how much adding or removing clusters from a clustering solution changes the objective function's value, as well as the notion of separability, which describes how far away clusters of points are from one another.  By characterizing point sets in terms of these statistics, the authors derive an algorithm for efficiently clustering the points and derive guarantees on the cluster quality.
\subsection{Definitions}
The paper begins by describing a few statistics on the point distributions which are necessary for the subsequent analysis.  Let $OPT_i$ be the contribution to the objective function of the optimal clustering by cluster $i$, and let $OPT^{i \rightarrow j}$ denote the the cost when cluster $i$ is removed from the $OPT$ solution (which, for simplicity's sake, we will assume to be unique) and its points are then given to cluster $j$.  Intuitively, $OPT^{i \rightarrow j}$ corresponds to a (bad) solution of a $k-1$ clustering, and $OPT(k)^{i \rightarrow j} \geq OPT(k-1)$, where $OPT(f)$ is the optimal solution of the $f$-cluster problem.  Let a clustering be $(1 + \alpha)$ weak-deletion stable if, $\forall i \neq j, OPT^{i \rightarrow j} > (1 + \alpha)OPT$ - in other words, removing any cluster and remapping those points to another single cluster severely contributes to the objective function, suggesting that that each cluster is significant to the solution. Intuitively if this is not the case, then one could reasonably use the ($k-1$) solution in place of the $k$ solution.

  We define an optimal clustering solution $C^*$ to be $\beta$-distributed for $\beta > 0$ if for any $i$ and any point $x \notin C^*_i, d(x, c^*_i) \geq \beta \frac{OPT}{|C^*_i|}$. Intuitively, this provides a factor $\beta$ for the distance that each point is from another cluster's center, relative to the average distance a point is from its own cluster center. Specifically for this paper, we will focus on the $k$-median case, but analogous logic applies to the $k$-means case as well.   

Now, we prove that any $(1+\alpha)$-weakly deletion-stable $k$-median instance is $\frac{\alpha}{2}$-distributed,

% and that any $(1+\alpha)$-weakly deletion-stable $k$-mean instance is $\frac{\alpha}{4}$-distributed.

In each case, the proof will examine the difference $OPT^{i \rightarrow j} - OPT$.  Each point $x \in C^*_i$ contributes $d(x, c^*_i)$ to $\Phi$, and on deleting $C^*_i$ and remapping its points to $C^*_j$, contributes $d(x, c^*_j)$.  By the triangle inequality, $d(x, c^*_j) \leq d(x, c^*_i) + d(c^*_i, c^*_j)$. Thus, the difference in contribution to $\Phi$ is the difference in cost upon swapping the points between the two clusters, which is $ d(x, c^*_j)  - d(x, c^*_i) \leq d(c^*_i, c^*_j)$.  Summing up over the original cluster $C^*_i$,

\begin{align}
\sum_{x \in C^*_i} (d(x, c^*_j) )  - OPT_i \leq |C^*_i| d(c^*_i, c^*_j)
\end{align}

The quantity on the LHS represents the increase in cost, which we know is $ > \alpha OPT$ by $(1+\alpha)$-weakly deletion-stability. This means $\alpha OPT < |C^*_i| d(c^*_i, c^*_j)$, or $d(c^*_i, c^*_j) >\frac{\alpha OPT}{|C^*_i|}$.  Then, for any point $p \in C^*_j, d(c^*_i, c^*_j) \leq d(c^*_i, p) + d(c^*_j, p) \leq 2d(c^*_i, p)$. The former inequality is due to the triangle inequality.  Thus, applying this to the previous inequality, we get that $d(c^*_i, p) > \frac{\alpha OPT}{2 |C^*_i|}$; i.e. $\frac{\alpha}{2}$ distributed.

\subsubsection{Relationship to the $(c, \delta)$-property}

We show that a k-median cluster with ($1+\alpha, \delta$) stability in which all clusters have size greater than $\delta n$ have $(1+\alpha)$ weak deletion-stability.

We use $C^{(i\rightarrow j)}$ to indicate the clustering obtained by taking all the points in cluster $C_i^*$ and moving them to cluster $C_j^*$. Because every cluster has at least $\delta n$ points, and $C^{(i \rightarrow j)}$ has one fewer cluster than $C^T$ we know the distance from $C^T$ and $C^{(i\rightarrow j)}$ is greater than $\delta$. Therefore the cost of $C^{(i\rightarrow j)}$ is at least $(1 + \alpha)\text{OPT}$.

\subsubsection{Inner Ring}


Define a \emph{cheap cluster} $C^*_i$ as one where $OPT_i \leq \frac{\beta \epsilon OPT}{32}$.  Let the \emph{inner ring} of $C_i$ be the set of points $x$ such that $d(x, c_i) \leq \frac{ \beta OPT }{8 |C^*_i|}$.  If $C_i$ is cheap, then no more than $\frac{\epsilon}{4}$ points reside outside its inner ring.  By contradiction, assume that more than $\frac{\epsilon}{4}$ fraction of points exist outside of the inner ring.  $OPT_i > (\text{points outside inner ring}) * (\text{cost of points})$, and the first term in that product is lower bounded by $\frac{\epsilon |C^*_i|}{4}$ and that second term is lower bounded by $\frac{\beta OPT}{8 |C^*_i|}$.  Thus, $OPT_i > \frac{\beta \epsilon OPT }{32}$.  But all cheap clusters have cost less than $\frac{\beta \epsilon OPT}{32}$, so we have a contradiction, and thus cheap clusters only have at most $\frac{\epsilon}{4}$ points outside their inner rings.

%TODO: what on Earth does Markov's inequality have to do with this?  Is this a typo in the paper?

%Yeah i have no idea... what you said seems uhh... pretty straightfoward

\subsection{Algorithm}
We'll begin by briefly discussing the intuition of the $(1 + \epsilon)$ approximation algorithm.

To complete the notion of a cheap cluster, we define an expensive cluster to be one with $\text{OPT}_i > \frac{\beta\epsilon\text{OPT}}{32}$. The key idea is to assume (and later we prove) that the number of expensive clusters is \emph{bounded}.  We then have a method for clustering the cheap point sets, assuming we can partition cheap and expensive points: guess the number of points in the cheap clusters, derive a relationship from the number of points to the radius $r$ of that point set, and then draw balls of radius $r$ around the points in each of those sets.  In order to cluster the expensive points, we then use brute force to guess each of the remaining cluster centers, and since the number of remaining clusters is bounded and small, this brute force operation should not contribute significantly to the runtime.  This all hinges on being able to partition the points into cheap and expensive sets, which arises naturally out of the procedure.  In practice, the algorithm attempts to create a list of possible subsets of points (which could overlap in membership), and iteratively refine that list to create our $k$ clusters. 



We provide the algorithm in fig (TODO), and give a very high level overview of the two main "stages" included.

First, the "Population stage."  We assume that we have some initial list $Q_{init}$ of graphs $\{T_i\}$ which is just the centers of the expensive clusters (i.e. one point per $T_i$).  Let $n$ be the number of datapoints.  We will count backwards (counter $s$) from $s=n$ down to $1$, i.e. once for each datapoint.

Now, let $d(x, Q)$ be the minimum distance from point $x$ to any point in any of the components in $Q$.  Now, let $r = \frac{\beta OPT}{4 s}$, where $r$ is the aforementioned ball radii; in other words, we're attempting to slowly grow the ball sizes with each iteration.  If a point is within $2r$ of $Q$, we remove it. Then, we loop again over each remaining point.

For each point $x$, look only at the set of points within $r$ of that point (including $x$ itself), and call this $B$.  Then, for each pair $(a, b)$ of points in $B$, we connect them if their distance is bounded by $r$ and if there are at least $\frac{s}{2}$ points in $B$ distance $r$ from both $a$ and $b$.

Finally, for each connected component that results this way, add it to $Q$ if it has at least $\frac{s}{2}$ points, and remove all the points of distance $2r$ from this component.  This ends the population stage.

%TODO: What happens to the "removed" points?
%Nothing, they just aren't possible candidates for cluster centers.

At the end of the population stage, we have a collection of connected components in $Q$ from which we derive clusters, in the "Centers-Retrieving stage."

In this stage, we continually select candidates of $k$ components from $Q$.  For each component, we choose the point that is the best possible center $c_i$ for that component and all points within $2r$ of that component.  Then, we assign all points to the nearest cluster center $c_i$.  Eventually, one of these subsets of components should give us a clustering with value at most $(1 + \epsilon) OPT$ (assuming $OPT$ is known; we will prove this).

\subsection{Proof of Correctness}

TODO: dscribe how we assume OPT is known and why that's valid.

\subsubsection{Points in components are either close or far to their centers}
We now move into the proof of correctness, and the paper starts by proving a useful lemma for the correctness of the population stage.  In this section, we only consider operations on clusters $C^*_i$ which are cheap at early steps such that $s \geq |C^*_i|$.  We prove two useful facts.

\begin{lemma}
Any component $T$ added in step $s$ does not contain any point $z$ such that $\frac{\beta OPT}{2 C^*_i} \leq  d(c_i, z) \leq \frac{3 \beta OPT}{4 C^*_i}$.
\label{lemma:4.2a}
\end{lemma}

\begin{proof}
By contradiction - assume that the distance of $z$ does fall within this range.  In the algorithm, $r = \frac{\beta OPT}{4 s}$, and because of the bound we imposed on $s$, $r = \frac{\beta OPT}{4 |C^*_i|}$.  Let $p$ be a point such that $d(p, z) < r$.  If $p$ is part of $C^*_i$ but falls outside of the inner ring (we'll show these two facts soon), and $T$ is added to $Q$, then by step d), it must be that $T$, which is in the ball $r$ of $z$ by step c), has more than $\frac{s}{2}$, and therefore more than $\frac{C_i}{2}$ points.  But, this is a contradiction, since more than half (at least $1 - \frac{\epsilon}{3}$) the points of any inner ring must fall \emph{within} the inner ring, not outside the inner ring, as it would here.

Now, we show that $p$ is part of $C^*_i$ and falls outside the inner ring, completing the proof.  

First, $p$ is part of $C^*_i$.  Since $d(c_i, z) \leq \frac{3 \beta OPT}{4 |C^*_i|}$ and, still, $d(z, p) < r \leq \frac{\beta OPT}{4 |C^*_i|}$, by the triangle inequality, $d(c_i, p) \leq d(c_i, z) + d(z, p) < \frac{\beta OPT}{|C^*_i|}$, which means that $p$ belongs to $C^*_i$ since our clustering is $\beta$-distributed.

Second, $p$ falls outside the inner ring.  Since $d(c_i, z) \geq \frac{\beta OPT}{2 |C^*_i|}$ and $d(z, p) < r \leq \frac{\beta OPT}{4 |C^*_i|}$, then by the triangle inequality, $d(c_i, p) \geq d(c_i, z) - d(z, p) > \frac{\beta OPT}{4 |C^*_i|}$. Thus, by the definition of an outer ring, $p$ must fall \emph{outside} it.

\end{proof}

\begin{lemma}
Given the same preconditions, $T$ cannot contain a pair of points $(p_1, p_2)$ such that $p_1$ is within $\frac{\beta OPT}{2 |C^*_i|}$ of $c_i$ and $p_2$ is greater than $\frac{4 \beta OPT}{3 |C^*_i|}$.
\label{lemma:4.2b}
\end{lemma}

\begin{proof}
Proof by contradiction.  Assume this did not hold.  Since we assume that $T$ is a connected component, there's a path from $p_1$ to $p_2$.  By step $c$ in the algorithm, two points are only connected if their distance is at most $r = \frac{\beta OPT}{4s} \leq \frac{\beta OPT}{4|C^*_i|}$.

However by triangle inequality $d(p_1, p_2) \geq d(p_2, c_i) - d(p_1, c_i) \geq \frac{5\beta OPT}{6|C_i|} \geq \frac{\beta OPT}{4|C^*_i|}$, leading to a contradiction. 

%  Then, since we assumed this lemma did not hold, $p_1$ is within $\frac{\beta OPT}{2 |C_i|}$ of $c_i$ and $p_2$ is greater than $\frac{4 \beta OPT}{3 |C_i|}$.  This means there can be a point that falls within that range along the connected path, which would contradict lemma \ref{lemma:4.2a}.
\end{proof}

Combined, these two lemmas state that every point in a component either lies relatively far (more than $\frac{4 \beta OPT}{3 |C^*_i|}$ )to $c_i$ or very close to $c_i$ (within $\frac{\beta OPT}{2 |C^*_i|}$).  

%TODO: is this accurate?
%Lemma 2.1 had a couple of clear bugs (proofs for inner ring and part of C_i were reversed) Reread 2.2 and see if I what I said made sense?

\subsubsection{Points in inner rings are quickly added for each cluster}

Here we show that for each cluster $C^*_i$, $Q$ will always contain a component $T'$ with a point from the inner ring of $C^*_i$ by step $s = |C^*_i|$.

To see why this is the case, assume that we are at step $s$ and the algorithm has not yet inserted some $T'$ yet.  There are two possible things that could happen here.  Either that component might be added at step $s$ (in which case we're done), or it could be that the point the algorithm would add has been removed by some other step.  What we need to prove is that even if an inner ring point is removed, the algorithm will still add a different inner ring point at this step.

Assume for contradiction's sake that no other inner ring point for $C^*_i$ is added at or before step $s$, and let $s' \geq |C^*_i|$ (remember we are counting \emph{backwards}) be the step at which the original inner ring point (call it $x$) were removed.  Then if this point were removed it's due to step b), and it was removed because there already exists a point (call it $y$) in a component in $Q$ within $2r = \frac{\beta OPT}{2s'} \leq \frac{\beta OPT}{2|C^*_i|}$ of this $x$.  Then, by the triangle inequality, since we also know that $d(c_i, x) \leq \frac{\beta OPT}{8 |C^*_i|}$ since it's an inner ring point,  $d(c_i, y) \leq d(c_i, x) + d(x, y) \leq \frac{\beta OPT}{2s} + \frac{\beta OPT}{8 |C^*_i|} \leq \frac{5 \beta OPT}{8 |C^*_i|}$.  Now, let $s''$ be the step at which $T''$, the component which held $y$ was inserted.  We know that $d(c_i, y) < \frac{\beta OPT}{2 |C^*_i|}$   By step d) in the algorithm, $T''$ must be of at least size $\frac{s''}{2}$ and thus at least size $\frac{|C^*_i|}{2}$, but by our assumption (for contradiction) does not contain a single inner ring point on $C^*_i$.  Then, since we know at least half of a cheap cluster should belong to its inner ring, all of the points in $T''$ can't be in $C^*_i$ and so at least one point (say, $w$) must be in some $C^*_j~i \neq j$.  By $\beta$ distribution, $d(c_i, w) > \frac{\beta OPT}{|C^*_i|}$, but by \ref{lemma:4.2b}, we can't have this far $w$ and close $y$ both in $T''$.  Thus we have a contradiction, and so a different inner ring point than $x$ must have already been added.

\subsubsection{A given component uniquely contains the inner ring of a given $C^*_i$}

Consider that $T$ contains the inner ring of some $C^*_i$ (which was established above). We show here that all points in $T$ are within $\frac{\beta\text{OPT}}{2|C^*_i|}$ of $c_i^*$, $T \cup B(T)$ are contained in $C_I^*$ and that no other component contains inner ring points from $C^*_i$.

We know all points in $T$ are within $\frac{\beta\text{OPT}}{2|C^*_i|}$ of $c_i^*$ because at least some are in the inner ring of $C^*_i$. Following from \ref{lemma:4.2a} and \ref{lemma:4.2b}, it is then impossible for a point to exist that is further than $\frac{\beta\text{OPT}}{2|C^*_i|}$ from $c_i^*$. This immediately implies that $T \subset C_i^*$ because $C_i^*$ is beta distributed. Additionally we have that all points in B(T) are less than $2r$ away from some point in $T$. Because we established $2r = \frac{\beta\text{OPT}}{4s} \leq \frac{\beta\text{OPT}}{2|C_i^*|}$, all points in B(T) then within $ \frac{\beta\text{OPT}}{|C_i^*|}$ of $c_i^*$, and therefor within $C_i^*$. 

Because all points in $T$ are sufficiently close to $C_i^*$ there are at most $|C_i^*|$ in $T$, and so $s < 2|C_i^*|$. All inner ring points are at most $\frac{\beta\text{OPT}}{4|C^*_i|} < 2r$ away from any other point, requiring all points not in $T$ be added to $B(T)$. Therefor no other component can contain inner ring points for $C_*^i$.


\subsubsection{Less than $16/(3\beta)$ that do not contain an inner ring point}

Consider a $T$ which does not contain an inner ring point. We will show that all points in $y \in T$ have distance greater than $\frac{3\beta\text{OPT}}{8s}$ to their optimal cluster centers $c_i^*$. Because each $T$ contains at least $s/2$ points, by doing so we show there cannot be more than $16/3\beta$ such $T$ (as their total contribution would then be OPT).

\begin{itemize}

\item Assume $y$ is from an expensive cluster $C^*_i$. Because $Q$ are the set of expensive clusters, we know that for the point to not be thrown out $d(c^*, y) > 2r = \frac{\beta\text{OPT}}{2s} > \frac{3\beta\text{OPT}}{8s}$.

\item $C^*_i$ is a cheaper cluster and $s \geq |C^*|$. Because we established $d(c^*, y) < \frac{\beta \text{OPT}}{2|C^*|}$ or $d(c^*, y) > \frac{3 \beta \text{OPT}}{4|C^*|}$ We know at least one point must meet the larger property. Since the inner ring contains $> |C^*|/2$, but $T$ also contains at least $s/2 = |C^*_i|/2$ points, some point in $T$ must not be in $C^*_i$, and therefor have distance larger than $\frac{3 \beta \text{OPT}}{4|C^*|}$. \ref{lemma:4.2b} then tells us all points must meet the property.

\item $C^*_i$ is a cheaper cluster and $s < |C^*|$. We know that all points are at least $2r$ from the center, as some such component meeting the conditions of the previous section must exist. $d(c^*, y) \geq d(x, y) - d(c^*, x) \geq \frac{\beta\text{OPT}}{2s} - \frac{\beta\text{OPT}}{8|C^*_i|} > \frac{3\beta\text{OPT}}{8s}$
\end{itemize}

\subsubsection{The algorithm outputs a k-clustering with cost $\leq (1+\epsilon)OPT$}

In order to show this we will compare the total cost of clusters $\mathcal{C}$ Derived by $T_1, ..., T_k$ with those of $\mathcal{C}^*$. We know that the expensive clusters $C_i$ have centers $c_i^*$, and so its total cost will not exceed $\text{OPT}_i$.

The cost of a cheap cluster is $\sum\limits_{x\in T\cup B(T)}d(x, c_i) + \sum\limits_{x \in C_i^* and x\notin T\cup B(T)}d(x, c_i)$. Because all points in the first term are in the innter ring, that includes the center, so $\sum\limits_{x\in T\cup B(T)}d(x, c_i) \leq \sum\limits_{x\in T\cup B(T)}d(x, c^*_i)$. We must show the error in $\sum\limits_{x \in C_i^* and x\notin T\cup B(T)}d(x, c_i)$ is bounded by $(1+\epsilon)$.

We will show that $d(c_i, c_i*) < \epsilon\frac{\beta\text{OPT}}{8|C_i^*|}$ for a cheap cluster. We know by the definition of a cheap cluster that the cost is at most $(\epsilon/32)\beta\text{OPT}/|C_i^*|$. In order not exceed this bound, at least half of the points have distance at most $(\epsilon/16)\beta\text{OPT}/|C_i^*|$ from $c_i^*$. Also these are by definition in the inner ring. We know that at least half the points in a cheap cluster reside in the inner ring. If $d(c_i, c_i^*)  \geq \epsilon\frac{\beta\text{OPT}}{8|C_i^*|}$, then we require at least half of the points to contribute at least $(\epsilon/16)\beta\text{OPT}/|C_i^*|$ to the sum, which means some point in the inner ring must do so. Therefor $\sum\limits_{x\in T\cup B(T)}d(x, c_i)> OPT_i$, which contradicts the optimality of the inner ring points. 

We therefor for each $x \in C_i^* and x\notin T\cup B(T)$ we have $d(c_i, c_i^*) \leq \epsilon\frac{\beta\text{OPT}}{8|C_i^*|}$. So $d(x, c_i) \leq d(x, c_i^*) + d(c_i^*, c_i) \leq (1+\epsilon)d(x,c_i^*)$. And so each point adds at most a factor of $(1+\epsilon)$ to the error, bounding our total error to $(1+\epsilon)\text{OPT}$




%(TODO: why?  I don't understand that).
% if it's within frac{\beta OPT}{2|C_i|} of some x, and that x is within frac{\beta OPT}{8|C_i|}  of the center, by triangle inequality the thing you were quesitoning holds. 

\subsection{Runtime Analysis}

Step 2) in the algorithm is run $n$ times.  For each of those $n$ runs, we loop, potentially, over all $n$  points (2 c)), and then as we look for potentially connecting points, we loop over all $n$  points again (this happens similarly in d)), leading to $O(n^3)$ iterations.  Step 2) overall is dominated by the $O(n^3)$ time factor.

Step 3) iterates over $k^{O(\frac{1}{\beta})}$ cluster guesses, and for each of these is a simple $n^2$ procedure to find the cluster centers (iterating potentially over each point $p$, and then iterating over potentially each other point to calculate statistics to determine if $p$ is the median)  (3 b).  Subsequently, step 3 measures the cost of each of the resulting clusters, which takes $O(nk)$ time (iterate over $k$ potential clusters.  For each, iterate over up to $n$ points).  Thus, step 3) takes $O(n^2 + nk)$ iterations for each guess of $k$ components.  We need to do this $k^{O(\frac{1}{\beta})}$ times, making the runtime for step 3 $O((n^2 + nk)k^{O(\frac{1}{\beta})})$

Thus, the runtime for the entire algorithm is $O((n^2 + nk)k^{O(\frac{1}{\beta})} + n^3)$.  However, this is only for each time the algorithm is run.  In general, the algorithm must be run once for each possible guess of the expensive clusters, using a brute-force approach.  This requires $n^{O(\frac{1}{\beta e})}$ iterations.  Thus, the overall runtime is $O(((n^2 + nk)k^{O(\frac{1}{\beta})}+n^3)n^{O(\frac{1}{\beta e})}) = O(n^{O(1/\beta\epsilon)}k^{O(1/\beta)}$ .

\subsection{Removing the cluster size restriction}

Our analysis up until now has hinged on having all clusters of size at least $\delta n$. When lifting this restriction, there exists an algorithm which is instead $O(\delta/\alpha)$-close to the target, presented in \cite{firstpaper}. Because this proof is fundamentally different than those above, we will only sketch the proof for intuition. 

We define $w(x)$ to be the distance from a point $x$ to it's closest cluster center, and $w_2(x)$ to be the distance to the second closest center. Let $w = \frac{\text{OPT}}{n}$ and $d_crit = 2\frac{\alpha w}{10 \epsilon}$. Let $X_i$ be the set of all points in $C_i^*$ for which $w(x) < d_crit$ and $w_2(x) - w(x) \geq 5d_crit$. Intuitively the points in $X_i$ are those which are strongly in $C_i^*$, and so moving them to the second closest cluster is expensive.

A \textbf{threshold graph} $G_\tau = (S, E_\tau)$ to be that which has all pairs of points $(x, y)$ connected if $d(x, y) \leq \tau$. For a problem satisfying the $(1 + \alpha, \epsilon)$-property, set $\tau = 2d_{crit}$. This graph has the following properties:

\begin{itemize}
\item For all $x, y \in X_i$, $(x,y) \in E(G_\tau)$. That is, if two points are both much closer to one center than any other, they will have an edge in this graph.

\item If $x \in X_i$ and $y \in X_{j \neq i}$, the $(x,y) \notin E(G_\tau)$, and they share no neighbors. That is, if two points are tightly bound each to different clusters, they will not share edges in the threshold graph. 

The algorithm, taken directly from \cite{firstpaper} is: *algorithm, I'll format it like you do*

As $w$ may not be known directly, running this algorithm on a $\gamma$-approximation of $w$ gives a $O(\gamma\delta/\alpha)$ approximation of the optimal clustering. As noted above, for sake of brevity we exclude the proof for this algorithm. However intuitively the threshold graph groups points into cliques,. By virtue of how the graph was created, we guarantee that when a cluster is selected from the graph, only a bounded number of possibly incorrect points can be added to that cluster. Additionally total number of points that were in a clique which belonged to a different cluster is bounded, so the final cluster also has bounded error. 

\end{itemize}
\bibliography{biblio}{}
\bibliographystyle{plain}

\end{document}