%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Short Sectioned Assignment
% LaTeX Template
% Version 1.0 (5/5/12)
%
% This template has been downloaded from:
% http://www.LaTeXTemplates.com
%
% Original author:
% Frits Wenneker (http://www.howtotex.com)
%
% License:
% CC BY-NC-SA 3.0 (http://creativecommons.org/licenses/by-nc-sa/3.0/)
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%----------------------------------------------------------------------------------------
%	PACKAGES AND OTHER DOCUMENT CONFIGURATIONS
%----------------------------------------------------------------------------------------

\documentclass[paper=a4, fontsize=11pt]{scrartcl} % A4 paper and 11pt font size

\usepackage[T1]{fontenc} % Use 8-bit encoding that has 256 glyphs


%\usepackage{fourier} % Use the Adobe Utopia font for the document - comment this line to return to the LaTeX default
\usepackage[english]{babel} % English language/hyphenation
\usepackage{amsmath,amsfonts,amsthm} % Math packages
\usepackage{mathtools}
\DeclarePairedDelimiter\ceil{\lceil}{\rceil}
\DeclarePairedDelimiter\floor{\lfloor}{\rfloor}
\usepackage{lipsum} % Used for inserting dummy 'Lorem ipsum' text into the template
\usepackage[margin=1.25in]{geometry}


\usepackage{sectsty} % Allows customizing section commands
\allsectionsfont{\centering \normalfont\scshape} % Make all sections centered, the default font and small caps

\usepackage{fancyhdr} % Custom headers and footers
\pagestyle{fancyplain} % Makes all pages in the document conform to the custom headers and footers
\fancyhead{} % No page header - if you want one, create it in the same way as the footers below
\fancyfoot[L]{} % Empty left footer
\fancyfoot[C]{} % Empty center footer
\fancyfoot[R]{\thepage} % Page numbering for right footer
\renewcommand{\headrulewidth}{0pt} % Remove header underlines
\renewcommand{\footrulewidth}{0pt} % Remove footer underlines
\setlength{\headheight}{13.6pt} % Customize the height of the header

\numberwithin{equation}{section} % Number equations within sections (i.e. 1.1, 1.2, 2.1, 2.2 instead of 1, 2, 3, 4)
\numberwithin{figure}{section} % Number figures within sections (i.e. 1.1, 1.2, 2.1, 2.2 instead of 1, 2, 3, 4)
\numberwithin{table}{section} % Number tables within sections (i.e. 1.1, 1.2, 2.1, 2.2 instead of 1, 2, 3, 4)

\setlength\parindent{0pt} % Removes all indentation from paragraphs - comment this line for an assignment with lots of text

\usepackage{mathrsfs}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{bbm}
\usepackage{amssymb} 
\usepackage[mathscr]{euscript}
\usepackage{parskip}


\renewcommand{\vec}[1]{\mathbf{#1}}
\newcommand{\vecsf}[1]{\textsf{\textbf{#1}}}

%----------------------------------------------------------------------------------------
%	TITLE SECTION
%----------------------------------------------------------------------------------------

\newcommand{\horrule}[1]{\rule{\linewidth}{#1}} % Create horizontal rule command with 1 argument of height

\title{	
\normalfont \normalsize 
\textsc{18.409 - Algorithmic Aspects of Machine Learning} \\ [25pt] % Your university, school and/or department name(s)
\horrule{0.5pt} \\[0.4cm] % Thin top horizontal rule
\huge Final Project \\ % The assignment title
\horrule{2pt} \\[0.5cm] % Thick bottom horizontal rule
}

\author{Mike Salvato and Andrew Spielberg} % Your name

\date{\normalsize\today} % Today's date or a custom date

\begin{document}

\maketitle % Print the title

%----------------------------------------------------------------------------------------
%	PROBLEM 1
%----------------------------------------------------------------------------------------
\section{Introduction}

Here we review work done on approximations for clustering algorithms, focusing on the work of \cite{firstpaper} and \cite{secondpaper}. The essential goal is that various data points about objects of interest, we can group those data points by which object they belong to. For example we may have a collection of face images, collect relative low dimensional data about the images, then cluster this data to try to determine images came from the same people by clustering them. However the problem of clustering points given a distance metric in general NP-hard. Additionally analysis of clustering alone does not take into account the relationship between the clusters and the underlying object they belong to. For example even if the images were clustered optimally, that clustering may not correctly group the images by which person's face is in the image. The work here will focus on approximating the underlying object a data point belongs to, using clustering methods.

\subsection{Background}

In order to begin our analysis of clustering, we first define what clustering means exactly, and the relationship it holds to solving an underlying data analysis problem. 

\subsection{k-medians}
The algorithm we will focus on is the k-medians algorithm. The k-medians algorithm assumes a space $(X, d)$ for which $X$ is the set of possible points and distance $d : ({ X \choose 2}) \rightarrow R_{\geq 0}$ which satisfies the triangle inequality. We are then given a set of points $S \subset X$. The goal is to label the points in $S$ each into one of k clusters $C_1, C_2, ..., C_k$ with corresponding representative points $c_1, c_2, ..., c_k \in X$. This defines a clustering $C$. The optimal objective clustering is that which minimizes $\Phi(C) = \sum\limits_{i = 1}^k \sum_{x \in C_i} d(x, c_i)$.

Let the distance between two clusterings $dist(C, C')$ be the 
fraction of points for which those two clusterings disagree on labels under the minimizing bijection between cluster labels, and let $\text{OPT}_\Phi = \Phi(C^*)$

\subsection{($c, \delta$)-property}
Thus far we have only addressed clustering based on the data points, but not the underlying objectives interest. The \textbf{objective} function $\Phi(C)$ is the cost of a clustering based only on the distance between points. However the optimal \textbf{target} clustering is that which partitions the points best based on the underlying objects the data points are drawn from. In the ideal problem statement the optimal objective clustering $C^*$ and the optimal target clustering $C^T$ are the same. However in practice this is not usually the case. In order to be able to perform analysis on the quality of a clustering \cite{firstpaper} define the $(c, \delta)$-property.  If a clustering satisfies the $(c, \delta)$-property, then for all clusterings where $\Phi(C) \leq c*\text{OPT}_\Phi$, $C$ is also $\delta$-close to $C^T$. By analyzing problems instances where this holds true for some values of $c$ and $\delta$, we can generate approximate clustering schema to achieve approximation solutions to discovering the target clustering.

 A couple of useful properties:

\begin{itemize}

\item In the case where the target clustering $C^T$ isn't the optimal clustering $C^*$, these two clusterings are $\delta$-close.

\item Since $dist$ is a metric, it satisfies the triangle inequality, and therefore any clustering that that is within $(c, \delta)$ wrt $C_T$ also satisfies $(c, 2\delta)$ of the optimal clustering. %(TODO: explain out with math?) I don't think it's necessary.



\end{itemize}

% Do we need all these? They're basically the strawmen, so I'm not sure they're the best use of space.


%Following this section begins specific analyses of how the $(c, \delta)$ assumption leads to efficient algorithms for clustering for three commonly used objective functions ($k$-median, $k$-mean, $k$-min-sum).  However, before that, it's important to understand three very important limitations and implications of this setting.
%
%\begin{enumerate}
%\item $(c, \delta)$ properties do not imply $(kc, k\delta)$ properties in general for any of the three objective functions we'll analyze here.  To see this, let's look at a counterexample for the $k$-median example (TODO: should I generalize this?).  Let $1 \leq c_1 < c_2 $, and $\delta, \alpha > 0$.  Let the target clustering consist of a single cluster with $n(1 - 2\alpha$ points at a single location, and the remaining $k-1$ clusters each with $2\frac{\alpha n}{k-1}$ all at distance $1$ (TODO: draw a picture).  Each point in $C_1$ is greater than $c_2 n$ away from any other cluster.  Note that this clustering has objective $2\alpha n - (k - 1)$, in other words, all but one point in each cluster but $C_1$ contributes $1$ to the objective ($C_1$ contributes nothing since each point lies on the median).  Then, consider an alternate $c_2$ solution which splits $C_1$ into two clusters and merges clusters $C_2$ and $C_3$.  Since $n(\frac{1}{2} - \alpha)$ points from the first cluster are then misclassified (as it was split in two), clearly this has approximation error at least 
%$\frac{1}{n}$ of that, and in fact, strictly greater.  If, in our original setup, we set the distance between points \emph{between} any two clusters to be $D > 1$ (which is defined to give this latter clustering a $c_2 OPT$ objective value).  But then, consider that any $c_1$ approximation cannot merge clusters (TODO: note that c2 was defined as the minimum that has a merging of clusters, can be seen by symmetry)).  error on fewer than $\frac{2\alpha n}{k-1} < \delta n$ (TODO: why?). Therefore, this is less than $k$ when we set $\alpha$ (TODO: explain that this defines the family) such that $k > 1 + \frac{2\alpha}{\delta} $.
%
%TODO: draw this whole proof.
%
%
%
%\item $c$-approximations under the $(c, \delta)$ assumption for $c > 1$ are as hard as finding a $c$-approximation in general. TODO: proof
%
%\item Finally, finding a $c$-approximation under the $(1, \delta)$ assumption is NP-Hard. TODO: proof
%
%\end{enumerate}

\subsection{Goals}

The goal of this work is to develop a minimal runtime clustering algorithm with minimal constraints on the values of $c$ and $\delta$ as described above. It is known that a better than $1 + \frac{2}{\delta}$ approximation for k-medians is NP-hard \cite{thethingthepapercitesJMS02}. In this review we will show explore the following results:

\begin{itemize}

\item Assume we have a problem with a $(1+\alpha, \delta)$-property, and that all target clusters have size greater than $\delta n$. There exists a PTAS $\delta$-close to the target.

\item Assume that the (k-1)-medians optimal cluster is more expensive than the k optimal clustering by a factor of $(1+\alpha)$ where $\alpha > 0$. Then it is possible to achieve a ($1+\epsilon$) approximation to the optimal k-medians in time polynomial in $n$ and $k$, and exponential in $1/\epsilon$ and $1/\alpha$, where $n = |S|$.

\end{itemize}


\section{Stability yields a PTAS for k-Median and k-Means Clustering}

This paper looks at a few statistics about how points are distributed and how it relates to optimal clusterings.  In particular, it studies notions of "stability," which describes bounds on how much adding or removing clusters from a clustering solution changes the objective function's value, and separability, which describes how far away clusters of points are from one another.  By characterizing point sets in terms of these statistics, the authors derive an algorithm for efficiently clustering the points and derive guarantees on the cluster quality, as well as prove its correctness.

\subsection{Definitions}
The paper begins by describing a few statistics on the point distributions which are necessary for the subsequent analysis.  Let $OPT_i$ be the contribution to the objective function of the optimal clustering by cluster $i$, and let $OPT^{i \rightarrow j}$ denote the the cost when cluster $i$ is removed from the $OPT$ solution (TODO: unique?) and its pointed are then given to cluster $j$.  Intuitively, $OPT^{i \rightarrow j}$ corresponds to a (bad) solution of a $k-1$ clustering, and $OPT(k)^{i \rightarrow j} \geq OPT(k-1)$, where $OPT(f)$ is the optimal solution of the $f$ cluster problem.  Let a clustering be $(1 + \alpha)$ weak-deletion stable if, $\forall i \neq j, OPT^{i \rightarrow j} > (1 + \alpha)OPT$ - in other words, removing any cluster and remapping those points to another single cluster severely contributes to the objective function, suggesting that that each cluster is significant to the solution. Intuitively if this is not the case, then one could reasonably use the ($k-1$) solution in place of the $k$ solution.

  We define an optimal clustering solution $C^*$ to be $\beta$-distributed for $\beta > 0$ if for any $i$ and any point $x \notin C^*_i, d(x, c^*_i) \geq \beta \frac{OPT}{|C^*_i|}$. Intuitively, this provides a factor $\beta$ for the distance that each point is from another cluster's center, relative to the average distance a point is from its own cluster center. Specifically for this paper, we will focus on the $k$-median case, but analogous logic applies to the $k$-means case as well.   

Now, we prove that any $(1+\alpha)$-weakly deletion-stable $k$-median instance is $\frac{\alpha}{2}$-distributed,

% and that any $(1+\alpha)$-weakly deletion-stable $k$-mean instance is $\frac{\alpha}{4}$-distributed.

In each case, the proof will examine the difference $OPT^{i \rightarrow j} - OPT$.  Each point $x \in C^*_i$ contributes $d(x, c^*_i)$ to $\Phi$, and on deleting $C^*_i$ and remapping its points to $C^*_j$, contributes $d(x, c^*_j)$.  By the triangle inequality, $d(x, c^*_j) \leq d(x, c^*_i) + d(c^*_i, c^*_j)$. Thus, the difference in contribution to $\Phi$ is the difference in cost upon swapping the points between the two clusters, which is $ d(x, c^*_j)  - d(x, c^*_i) \leq d(c^*_i, c^*_j)$.  Summing up over the original cluster $C^*_i$,

\begin{align}
\sum_{x \in C^*_i} (d(x, c^*_j) )  - OPT_i \leq |C^*_i| d(c^*_i, c^*_j)
\end{align}

The quantity on the LHS represents the increase in cost, which we know is $ > \alpha OPT$ by $(1+\alpha)$-weakly deletion-stability. This means $\alpha OPT < |C^*_i| d(c^*_i, c^*_j)$, or $d(c^*_i, c^*_j) >\frac{\alpha OPT}{|C^*_i|}$.  Then, for any point $p \in C^*_j, d(c^*_i, c^*_j) \leq d(c^*_i, p) + d(c^*_j, p) \leq 2d(c^*_i, p)$. The former inequality is due to the triangle inequality.  Thus, applying this to the previous inequality, we get that $d(c^*_i, p) > \frac{\alpha OPT}{2 |C^*_i|}$; i.e. $\frac{\alpha}{2}$ distributed.

\subsubsection{Relationship to the $(c, \delta)$-property}

We show that a k-median cluster with ($1+\alpha, \delta$) stability in which all clusters have size greater than $\delta n$ have $(1+\alpha)$ weak deletion-stability.

We use $C^{(i\rightarrow j)}$ to indicate the clustering obtained by taking all the points in cluster $C_i^*$ and moving them to cluster $C_j^*$. Because every cluster has at least $\delta n$ points, and $C^{(i \rightarrow j)}$ has one fewer cluster than $C^T$ we know the distance from $C^T$ and $C^{(i\rightarrow j)}$ is greater than $\delta$. Therefore the cost of $C^{(i\rightarrow j)}$ is at least $(1 + \alpha)\text{OPT}$.

\subsection{Algorithm}
First, like in the paper, we'll begin by briefly discussing the intuition of the algorithm.

Let's begin by partitioning our set of clusters into two types.  There are  "cheap" clusters and "expensive" clusters, with the idea being that the expensive clusters contribute a relatively much larger amount than the cheaper clusters; in other words, we have mostly densely packed groups of points, and a few not-so densely packed groups.  The key idea is to assume (and later we prove that the assumption is valid) that the number of expensive clusters is \emph{bounded}.  We then have a method for clustering the cheap point sets, assuming we can partition cheap and expensive points - guess the number of points in the cheap clusters, derive a relationship from the number of points to the radius $r$ of that point set, and then draw balls of radius $r$ around the points in each of those sets.  In order to cluster the expensive points, we then use brute force to guess each of the remaining cluster centers, and since the number of remaining clusters is bounded and small, this brute force operation should not contribute significantly to the runtime.  This all hinges on being able to partition the points into cheap and expensive sets, which arises naturally out of the procedure.  In practice, the algorithm attempts to create a list of possible subsets of points (which could overlap in membership), and iteratively refine that list to create our $k$ clusters.

TODO: verify all this.

Because we assume the clustering is $\beta$-distributed, 


\end{document}